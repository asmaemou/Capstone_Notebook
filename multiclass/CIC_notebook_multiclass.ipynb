{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import make_scorer, accuracy_score, recall_score, precision_score, f1_score, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the datasets into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fridayAF_DDos = pd.read_csv(\"../datasets/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")\n",
    "df_fridayAF_PortScan = pd.read_csv(\"../datasets/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\")\n",
    "df_fridayMO = pd.read_csv(\"../datasets/Friday-WorkingHours-Morning.pcap_ISCX.csv\")\n",
    "df_monday = pd.read_csv(\"../datasets/Monday-WorkingHours.pcap_ISCX.csv\")\n",
    "df_thursdayAF_Infilteration = pd.read_csv(\"../datasets/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\")\n",
    "df_thursdayMO_WebAttacks = pd.read_csv(\"../datasets/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\")\n",
    "df_tuesday = pd.read_csv(\"../datasets/Tuesday-WorkingHours.pcap_ISCX.csv\")\n",
    "df_wednesday = pd.read_csv(\"../datasets/Wednesday-workingHours.pcap_ISCX.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating the dataframes to single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.concat([df_fridayAF_DDos,df_fridayAF_PortScan,df_fridayMO,df_monday,df_thursdayAF_Infilteration,df_thursdayMO_WebAttacks,df_tuesday,df_wednesday], axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying unique values from the 'Label' column\n",
    "unique_attacks = df_data[' Label'].unique()\n",
    "print(unique_attacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before applying labellization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[' Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_map = {\n",
    "    'Web Attack � Brute Force': 'Web Attack',\n",
    "    'Web Attack � Sql Injection': 'Web Attack',\n",
    "    'Web Attack � XSS': 'Web Attack',\n",
    "    'FTP-Patator': 'Brute Force',\n",
    "    'SSH-Patator': 'Brute Force',\n",
    "    'DoS slowloris': 'DoS',\n",
    "    'DoS Slowhttptest': 'DoS',\n",
    "    'DoS Hulk': 'DoS',\n",
    "    'DoS GoldenEye': 'DoS',\n",
    "    'DDoS': 'DoS',\n",
    "    'PortScan': 'PortScan',\n",
    "    'Bot': 'Bot',\n",
    "    'Infiltration': 'Infiltration',\n",
    "    'Heartbleed': 'Heartbleed',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[' Label'] = df_data[' Label'].replace(attack_map)\n",
    "\n",
    "# Now you can print the DataFrame to see the changes\n",
    "print(df_data[' Label'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_data is your DataFrame and it's already been modified with the attack_map\n",
    "\n",
    "# Create a count plot for the 'Label' column\n",
    "plt.figure(figsize=(6, 8))  # Adjust the size of the plot as needed\n",
    "sns.countplot(y=' Label', data=df_data, order=df_data[' Label'].value_counts().index)\n",
    "plt.title('Distribution of Attack Types')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Attack Type')\n",
    "# plt.grid(True)  # Optionally add a grid for better readability\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After applying labellization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = df_data[' Label'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find columns with zero variance; columns where all values are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_variance_cols = [col for col in df_data.columns if df_data[col].nunique() == 1]\n",
    "\n",
    "# Display the columns with zero variance\n",
    "if zero_variance_cols:\n",
    "    print(f\"Columns with zero variance: {zero_variance_cols}\")\n",
    "else:\n",
    "    print(\"No columns with zero variance found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shape before removing zero variance columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape before removing zero variance columns:', df_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle columns with zero variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if zero_variance_cols:\n",
    "        df_data.drop(zero_variance_cols, axis=1, inplace=True)\n",
    "        print(f'Dropped zero variance columns: {zero_variance_cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shape after removing zero variance columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape after removing zero variance columns:', df_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find  spaces from column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handle spaces from column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.columns = df_data.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Rows with NaN, inf, or -inf Values\n",
    "##### The row listed here contain  NaN ( Not a number) across the columns displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "numeric_cols = df_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Check for inf and -inf values in the numeric columns\n",
    "inf_values = np.isinf(numeric_cols).sum()\n",
    "\n",
    "# Display the count of inf and -inf values in each numeric column\n",
    "print(\"Count of inf and -inf values in each numeric column:\")\n",
    "print(inf_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how many NaN it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace inf and -inf with Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for empty strings or negative values in Flow Bytes/s or Flow Packets/s since they both contains the same number of NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = df_data[df_data['Flow Bytes/s'].isna() | (df_data['Flow Bytes/s'] == '') | (df_data['Flow Bytes/s'] < 0)]\n",
    "sub_df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check for NaN values in Label column:\", df_data['Label'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function will filter the dataframe based on NaN values in the specified columns and then calculate the value counts of the 'Label' column for these filtered rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def nan_label_counts(df_data, column_nan_list):\n",
    "    # Create a mask that is True wherever any of the columns in column_nan_list have NaN values\n",
    "    nan_mask = df_data[column_nan_list].isna().any(axis=1)\n",
    "    \n",
    "    # Filter the dataframe based on the NaN mask\n",
    "    nan_rows = df_data[nan_mask]\n",
    "    \n",
    "    # Return the value counts of the 'Label' column in the filtered dataframe\n",
    "    return nan_rows['Label'].value_counts()\n",
    "\n",
    "# Example usage:\n",
    "column_nan_list = ['Flow Bytes/s']\n",
    "print(nan_label_counts(df_data, column_nan_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def nan_label_counts(df_data, column_nan_list):\n",
    "    # Create a mask that is True wherever any of the columns in column_nan_list have NaN values\n",
    "    nan_mask = df_data[column_nan_list].isna().any(axis=1)\n",
    "    \n",
    "    # Filter the dataframe based on the NaN mask\n",
    "    nan_rows = df_data[nan_mask]\n",
    "    \n",
    "    # Return the value counts of the 'Label' column in the filtered dataframe\n",
    "    return nan_rows['Label'].value_counts()\n",
    "\n",
    "# Example usage:\n",
    "column_nan_list = ['Flow Packets/s']\n",
    "print(nan_label_counts(df_data, column_nan_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the occurences of Nan are very low, dropping the Nan values will not have any bad effect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = df_data[df_data.duplicated()]\n",
    "\n",
    "if not duplicate_rows.empty:\n",
    "    print(\"Duplicate rows:\")\n",
    "    print(duplicate_rows.shape)\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify columns with identical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_pairs = []\n",
    "num_columns = len(df_data.columns)\n",
    "\n",
    "for i in range(num_columns):\n",
    "    for j in range(i + 1, num_columns):\n",
    "        if df_data.iloc[:, i].equals(df_data.iloc[:, j]): \n",
    "            column_pairs.append((df_data.columns[i], df_data.columns[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape before removing identical columns:\", df_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print the column pairs with identical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if column_pairs:\n",
    "    print(\"Columns with identical values:\")\n",
    "    for pair in column_pairs:\n",
    "        print(f\"{pair[0]} and {pair[1]} have identical values.\")\n",
    "    \n",
    "    # Step 3: Drop one column from each pair\n",
    "    columns_to_drop = [pair[1] for pair in column_pairs]\n",
    "    df_data.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "    print(f\"Dropped columns: {columns_to_drop}\")\n",
    "else:\n",
    "    print(\"No columns with identical values found.\")\n",
    "\n",
    "print(\"Shape after removing identical columns:\", df_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for non-numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_columns = df_data.select_dtypes(include=['object']).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying the unique categories of cyberattacks from a dataset, represented by the 'Label' column in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data['Label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Network Traffic Types in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'df_data' is your DataFrame and it has a column 'Label' with all attack types\n",
    "sns.countplot(y='Label', data=df_data, order=df_data['Label'].value_counts().index)\n",
    "plt.title('Distribution of Attack Types')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Attack Type')\n",
    "plt.show()\n",
    "print(df_data['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a heatmap to visualize missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df_data.isnull(), cbar=False, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color: The uniform color (purple in this case) across the entire heatmap indicates consistency in the data regarding the presence or absence of missing values. Since the heatmap shows no variation in color, it tells us that there are no missing (NaN) values in any part of your DataFrame.\n",
    "### Vertical Axis: Since our DataFrame likely contains a large number of rows, only some index labels are shown\n",
    "### Horizontal Axis (X-axis): Displays the feature or column names of your DataFrame. All columns are represented, and the uniformity of color across all columns confirms that no columns contain missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative check to confirm that there are indeed no missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_missing = df_data.isnull().sum().sum()\n",
    "print(\"Total missing values in the dataset:\", total_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Categorical Features Based on Unique Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = []\n",
    "\n",
    "for c in df_data.columns:\n",
    "    # Check if the column is categorical (dtype of object or category) or numerical with less than 10 unique values\n",
    "    if df_data[c].nunique() < 10:\n",
    "        # Print unique values of columns with < 10 unique values\n",
    "        print(f\"Column '{c}' has unique values: {df_data[c].unique()}\")\n",
    "        categorical_columns.append(c)\n",
    "\n",
    "# If you want to display the list of categorical columns with unique values < 10\n",
    "print(\"Columns with unique values < 10:\")\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create numerical columns list and remove the target from categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### categorical columns before removing Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### categorical columns after removing Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [col for col in df_data.columns if col not in categorical_columns]\n",
    "categorical_columns.remove('Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[categorical_columns].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution : Check the distribution of the target variable (Label) to identify whether the data is balanced or imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'df_data' is your DataFrame and it has a column 'Label' with all attack types\n",
    "sns.countplot(y='Label', data=df_data, order=df_data['Label'].value_counts().index)\n",
    "plt.title('Distribution of Attack Types')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Attack Type')\n",
    "plt.show()\n",
    "print(df_data['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In our exploratory data analysis, we identified that our dataset is significantly imbalanced, with a larger number of 'Benign' instances compared to all other attacks. This imbalance can lead to biased predictive models that perform well on the majority class but poorly on the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix : Blue represents negative correlations, red represents positive correlations, and white or pale colors represent no correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = df_data[numerical_columns].corr()\n",
    "\n",
    "# Plotting the heatmap with adjustments\n",
    "plt.figure(figsize=(12, 10)) \n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', linewidths=0.5)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since there are too many features, I focused on the most correlated features by selecting a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top correlated features (optional)\n",
    "corr_threshold = 0.5  # Example: Use a threshold to filter features\n",
    "high_corr_features = corr_matrix.columns[(corr_matrix.abs() > corr_threshold).any()].tolist()\n",
    "\n",
    "# Plot only the high-correlation features\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix.loc[high_corr_features, high_corr_features], annot=False, cmap='coolwarm', linewidths=0.5)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.title('High-Correlation Features Heatmap', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since there are too many features, I focused on the most correlated features by selecting a subset and use a threshold to filter features. This particular heatmap visualizes the relationships among various network traffic features, which are crucial for identifying patterns that might suggest normal or malicious behaviors in network traffic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular summary of the most and least correlated features from a correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_correlations(corr_matrix, n=5):\n",
    "    # Create a DataFrame from the lower triangle of the correlation matrix, excluding the diagonal\n",
    "    corr_df = corr_matrix.where(np.tril(np.ones(corr_matrix.shape), k=-1).astype(np.bool))\n",
    "    \n",
    "    # Stack the DataFrame and reset the index to turn it into a long format\n",
    "    long_corr = corr_df.stack().reset_index()\n",
    "    long_corr.columns = ['Feature1', 'Feature2', 'Correlation']\n",
    "\n",
    "    # Get the top n most correlated features\n",
    "    top_correlations = long_corr.nlargest(n, 'Correlation')\n",
    "    \n",
    "    # Get the top n least correlated features\n",
    "    least_correlations = long_corr.nsmallest(n, 'Correlation')\n",
    "\n",
    "    return top_correlations, least_correlations\n",
    "\n",
    "# Get the summaries\n",
    "top_correlations, least_correlations = summarize_correlations(corr_matrix, n=5)\n",
    "\n",
    "# Print or return these summaries as needed\n",
    "print(\"Top Correlations:\")\n",
    "print(top_correlations)\n",
    "print(\"\\nLeast Correlations:\")\n",
    "print(least_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plot to Check Outliers: Box plots help identify outliers within the dataset for each feature. Box plots for each numerical column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "df_data[numerical_columns].boxplot(rot=90)\n",
    "plt.title(\"Box Plot for Outlier Detection\")\n",
    "plt.ylim(0, max(df_data[numerical_columns].max())*1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focus on the \"Flow Bytes/s\" column only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the shape before removing the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_of_interest = 'Flow Bytes/s'\n",
    "# Plot box plot for the selected column\n",
    "plt.figure(figsize=(8, 6))\n",
    "df_data[[column_of_interest]].boxplot()\n",
    "plt.title(f\"Box Plot for Outlier Detection: {column_of_interest}\")\n",
    "plt.ylim(0, df_data[column_of_interest].max() * 1.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Outliers : Capping the outliers instead of removing them ensures that extreme values are still present but limited to a reasonable range\n",
    "* The first quartile Q1 is the 25th percentile\n",
    "* The third quartile Q3 is the 75th percentile\n",
    "* Any data point that’s 1.5 points below the lower bound quartile or above the upper bound quartile is an outlier. \n",
    "### The cap_outliers doesn't remove the outliers but caps them meaing outliers are adjusted or limited to fall within a certain range. \n",
    "### Any value that exceeds a predefined upper or lower bound is set to that boundary value. It modifies values that fall outside these bounds by capping them at the nearest bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers(df, col):\n",
    "    Q1 = df[col].quantile(0.25) ##  The first quartile Q1 is the 25th percentile\n",
    "    Q3 = df[col].quantile(0.75) ## The third quartile Q3 is the 75th percentile\n",
    "    IQR = Q3 - Q1 ## The IQR is useful because it focuses on the middle 50% of the data and excludes the extreme values\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "    return df\n",
    "df_data = cap_outliers(df_data, 'Flow Bytes/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "df_data[[column_of_interest]].boxplot()\n",
    "plt.title(f\"Box Plot after Handling Outliers: {column_of_interest}\")\n",
    "plt.ylim(0, df_data[column_of_interest].max() * 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the shape after removing the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label encoder\n",
    "## print the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_data.iloc[:, -1] = le.fit_transform(df_data.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting of the data : HoldOut Validation Approach Train and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "X = df_data.drop('Label',axis=1)\n",
    "\n",
    "# Make sure to drop corresponding rows in y\n",
    "y = df_data['Label']\n",
    "\n",
    "# Split the dataset into train and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perform undersampling on BENIGN and DoS using K-means clustering. cluster-based instance selection (CBIS). Removing samples from the majority class\n",
    "dataset. \n",
    "* Perform oversampling on Infiltration and Heartbleed. SMOTE algorithm, which is a popular over-sampling technique\n",
    "* the majority class are BENIGN and DoS and the minority class are DoS,PortScan,Brute Force,Web Attack,Bot,Infiltration,Heartbleed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we don’t provide a specific target value, the resampling algorithm might either undersample too aggressively (leading to potential loss of important information) or not enough. \n",
    "### Specifying target sizes allows us to control the resampling process and ensure that each class is appropriately represented after resampling. \n",
    "* https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.ClusterCentroids.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assume X and y are predefined\n",
    "# Example:\n",
    "# X = pd.DataFrame(...)  # Feature matrix\n",
    "# y = pd.Series(...)     # Target vector\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Analyze original training class distribution\n",
    "original_counts_train = Counter(y_train)\n",
    "print(\"Original training class distribution:\", original_counts_train)\n",
    "\n",
    "# Step 2: Define sampling strategy\n",
    "majority_classes = [0, 3, 5]\n",
    "sampling_strategy = {}\n",
    "for cls in majority_classes:\n",
    "    if cls in original_counts_train:\n",
    "        desired_count = max(1, int(0.1 * original_counts_train[cls]))\n",
    "        sampling_strategy[cls] = desired_count\n",
    "    else:\n",
    "        print(f\"Class {cls} not found in the training target vector.\")\n",
    "\n",
    "print(\"Sampling strategy:\", sampling_strategy)\n",
    "\n",
    "# Step 3: Initialize ClusterCentroids\n",
    "cc = ClusterCentroids(\n",
    "    estimator=MiniBatchKMeans(n_init=1, random_state=0),\n",
    "    sampling_strategy=sampling_strategy,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Apply undersampling to training data\n",
    "X_train_resampled, y_train_resampled = cc.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 6: Verify resampled training class distribution\n",
    "new_counts_train = Counter(y_train_resampled)\n",
    "print(\"Resampled training class distribution:\", new_counts_train)\n",
    "\n",
    "# Step 7: Train the model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Step 8: Evaluate the model on the untouched test set\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i want to perform undersampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply oversampling to Infiltration and Heartbleed\n",
    "smote = SMOTE(sampling_strategy={'Infiltration': 1000, 'Heartbleed': 1000}, random_state=0)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the new class distribution\n",
    "print('Resampled dataset shape:', Counter(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "#rf.fit(X_train_downsampling, y_train)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Convert to a pandas DataFrame for easier visualization\n",
    "importance_series = pd.Series(feature_importances, index=X_train.columns)\n",
    "\n",
    "# Sort the features by importance\n",
    "importance_series = importance_series.sort_values(ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "importance_series.plot(kind='bar', color='skyblue')\n",
    "plt.title('Feature Importance Using RandomForest')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE should be applied after splitting the data into training and testing sets. This is to ensure that the synthetic data points created do not leak from the test set into the training set, which could lead to overfitting:\n",
    "How SMOTE works:\n",
    "\n",
    "1. Identifying Neighbors: For a data point in the minority class, SMOTE finds its nearest neighbors in the feature space.\n",
    "2. Synthesizing New Data: SMOTE selects one of these nearest neighbors and computes a line segment connecting the minority class data point and its selected neighbor. It then creates new, synthetic data points along this line.\n",
    "3. Repeating: This process is repeated until the minority class is adequately represented and balances the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle class imbalance : SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# # Drop rows with missing values\n",
    "# X = df_data.drop('Label', axis=1)\n",
    "# y = df_data['Label']\n",
    "\n",
    "# # Split the dataset into train and test sets (70% train, 30% test)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Applying SMOTE to the training data\n",
    "# smote = SMOTE(random_state=42,k_neighbors=2)\n",
    "# X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# X_train, y_train = X_train_smote, y_train_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Assume X and y are your features and labels\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Applying SMOTE to the training data\n",
    "# smote = SMOTE(random_state=42, k_neighbors=2)\n",
    "# X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Now, X_train and y_train are the resampled data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To see how the SMOTE impacted each feature in my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the importance for every feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate mutual information between numerical features and the target\n",
    "# importances = mutual_info_classif(X_train, y_train)\n",
    "\n",
    "# # Convert to a pandas DataFrame for easier visualization\n",
    "# mi_series = pd.Series(importances, index=X.columns)\n",
    "\n",
    "# # Sort the features by importance\n",
    "# mi_series = mi_series.sort_values(ascending=False)\n",
    "\n",
    "# # Plot the mutual information values\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# mi_series.plot(kind='bar', color='skyblue')\n",
    "# plt.title('Mutual Information Between Features and Label')\n",
    "# plt.xlabel('Features')\n",
    "# plt.ylabel('Mutual Information')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the sum of importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sort the feature importances along with the feature names\n",
    "# f_list = sorted(zip(map(lambda x: round(x, 4), importances), X_train.columns), reverse=True) ## \n",
    "\n",
    "# # Initialize variables\n",
    "# Sum = 0\n",
    "# fs = []\n",
    "\n",
    "# # Calculate the sum of feature importances and store features\n",
    "# for i in range(len(f_list)):\n",
    "#     Sum += f_list[i][0]  # Summing up the importance scores\n",
    "#     fs.append(f_list[i][1])  # Append the feature name to the list\n",
    "\n",
    "# # Optional: Print or return the feature importances and the sum\n",
    "# print(\"Sum of feature importances:\", Sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We select the important feature until the accumulated importance reaches 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_list2 = sorted(zip(map(lambda x: round(x, 4), importances/Sum), X_train.columns), reverse=True)\n",
    "# Sum2 = 0\n",
    "# fs = []\n",
    "\n",
    "# for i in range(0, len(f_list2)):\n",
    "#     Sum2 = Sum2 + f_list2[i][0]\n",
    "#     fs.append(f_list2[i][1])\n",
    "    \n",
    "#     if Sum2 >= 0.9:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(fs))\n",
    "# print(fs)\n",
    "feature_selected = ['Average Packet Size', 'Packet Length Mean', 'Fwd Header Length', 'Init_Win_bytes_forward', 'Packet Length Variance', 'Packet Length Std', 'Subflow Fwd Bytes', 'Total Length of Fwd Packets', 'Bwd Header Length', 'Flow IAT Max', 'Max Packet Length', 'Fwd IAT Max', 'Destination Port', 'Init_Win_bytes_backward', 'Flow Bytes/s', 'Fwd Packet Length Max', 'Flow Duration', 'Fwd IAT Total', 'Total Length of Bwd Packets', 'Subflow Bwd Bytes', 'Fwd Packets/s', 'Fwd Packet Length Mean', 'Bwd Packets/s', 'Avg Fwd Segment Size', 'Flow Packets/s', 'Flow IAT Mean', 'Fwd IAT Mean', 'Bwd Packet Length Mean', 'Avg Bwd Segment Size', 'Bwd Packet Length Max', 'Flow IAT Std', 'Fwd IAT Std', 'Total Backward Packets', 'Total Fwd Packets', 'Fwd Packet Length Std', 'Bwd IAT Total', 'Bwd IAT Max', 'act_data_pkt_fwd', 'Bwd IAT Mean', 'Bwd Packet Length Std', 'Bwd IAT Std', 'Fwd IAT Min', 'Flow IAT Min', 'Bwd IAT Min']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Z-Score Normalization : In our preprocessing steps, we applied Z-score normalization to our numerical features using StandardScaler. This method standardizes the features so that each has a mean of zero and a standard deviation of one, aligning with the Z-score formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler only on the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train[numerical_columns])\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test[numerical_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code handles the integration of scaled numerical data with unscaled categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the scaled data back into DataFrames for ease of use\n",
    "# columns=numerical_columns: This specifies the column names for the new DataFrame\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=numerical_columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=numerical_columns)\n",
    "\n",
    "# axis=1 means along \"columns\". It's a column-wise operation.\n",
    "X_train_final = pd.concat([X_train_scaled_df, X_train[categorical_columns].reset_index(drop=True)], axis=1)\n",
    "X_test_final = pd.concat([X_test_scaled_df, X_test[categorical_columns].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Print the shape of the final datasets to verify the concatenation\n",
    "print(\"X_train_final shape:\", X_train_final.shape)\n",
    "print(\"X_test_final shape:\", X_test_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell represent how data are represented before being scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This cell represent how data are represented after being scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models on all the features\n",
    "#### We perform k-fold cross validation on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []  # For normal training results\n",
    "score_hyper = []  # For hyperparameter tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_params_to_json(model_name, best_params):\n",
    "    filename= \"best_params_multiclass_multiple_models.json\"\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(filename):\n",
    "        # Load existing data\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    else:\n",
    "        # Create a new dictionary if the file does not exist\n",
    "        data = {}\n",
    "\n",
    "    # Update the dictionary with the new model's best parameters\n",
    "    data[model_name] = best_params\n",
    "\n",
    "    # Write updated dictionary back to the JSON file\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_fs_params_to_json(model_name, best_params):\n",
    "    filename= \"best_params_fs_multiclass_multiple_models.json\"\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(filename):\n",
    "        # Load existing data\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    else:\n",
    "        # Create a new dictionary if the file does not exist\n",
    "        data = {}\n",
    "\n",
    "    # Update the dictionary with the new model's best parameters\n",
    "    data[model_name] = best_params\n",
    "\n",
    "    # Write updated dictionary back to the JSON file\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)\n",
    "mlflow.set_experiment(\"training_by_default_multiclass\")\n",
    "mlflow.start_run(run_name=\"rfc_multi_model\",nested=True)\n",
    "# Fit the RandomForest model on the training data\n",
    "rfc.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold  Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  cross_validate\n",
    "\n",
    "# Define K-Fold cross-validator\n",
    "kfold_validation = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the classifier\n",
    "rf_cross_scores = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation with multiple scoring metrics\n",
    "rf_cross_scores = cross_validate(rf_cross_scores, X, y, cv=kfold_validation, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Print the scores for each fold and metric\n",
    "print(\"Average Accuracy Score:\", rf_cross_scores['test_score'].mean())\n",
    "print(rf_cross_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = rfc.predict(X_test_final)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "mcc = matthews_corrcoef(y_test, y_pred) # function calculates MCC by comparing the true labels (y_test) with the predicted labels (y_pred).\n",
    "\n",
    "score.append(['Random Forest', accuracy, recall, precision, f1, mcc])\n",
    "\n",
    "metrics = {\"accuracy\" : accuracy , \"recall\" :recall, \"precision\":precision,\"f1\":f1,\"mcc\":mcc}\n",
    "mlflow.log_metrics(metrics)\n",
    "mlflow.end_run()\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy * 100)\n",
    "print('Recall:', recall * 100)\n",
    "print('Precision:', precision * 100)\n",
    "print('F1 Score:', f1 * 100)\n",
    "print('MCC:', mcc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assume y_test and y_pred are defined elsewhere in your code\n",
    "# Compute the confusion matrix\n",
    "cm_rfc = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define the labels from your dataset (adjust labels as per your actual data)\n",
    "labels = ['BENIGN', 'DoS', 'PortScan', 'DDoS', 'Brute Force', 'Web Attack', 'Bot', 'Infiltration', 'Heartbleed']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_rfc, annot=True, cmap='Blues', fmt='g',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's proceed with RandomizedSearchCV\n",
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5001\")  # replace with your tracking URI\n",
    "\n",
    "# Define parameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=16,\n",
    "    scoring='f1_weighted',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "mlflow.set_experiment(\"hyperparameter_tuning_multiclass\")\n",
    "\n",
    "\n",
    "# automatically starts and ends an MLflow run\n",
    "mlflow.start_run(run_name=\"random_forest_tuned_multiclass\")\n",
    "# Fit the RandomForest model on the training data\n",
    "random_search.fit(X_train_final, y_train)\n",
    "\n",
    "# Save the best parameters\n",
    "model_name = \"Random Forest\"\n",
    "best_params = random_search.best_params_\n",
    "mlflow.log_params(best_params)\n",
    "\n",
    "# Optionally log the model as well\n",
    "mlflow.sklearn.log_model(random_search.best_estimator_, \"random_forest_model_mlflow\")\n",
    "\n",
    "\n",
    "save_best_params_to_json(model_name, best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test_final)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "score_hyper.append(['Random Forest (Tuned)', accuracy, recall, precision, f1, mcc])\n",
    "metrics = {\"accuracy\" : accuracy , \"recall\" :recall, \"precision\":precision,\"f1\":f1,\"mcc\":mcc}\n",
    "mlflow.log_metrics(metrics)\n",
    "mlflow.end_run()\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy * 100)\n",
    "print('Recall:', recall * 100)\n",
    "print('Precision:', precision * 100)\n",
    "print('F1 Score:', f1 * 100)\n",
    "print('MCC:', mcc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) are important tools for evaluating the performance of a binary classifier. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings, providing insight into the trade-off between sensitivity (true positive rate) and specificity (false positive rate). The AUC provides a single scalar value which measures the overall ability of the model to discriminate between the positive and negative classes across all possible thresholds. Essentially, a higher AUC value indicates better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Calculate predicted probabilities for the test data using the best model from hyperparameter tuning\n",
    "y_pred_prob_tuned = best_rf.predict_proba(X_test_final)[:, 1]  # Take the probability for the positive class (1)\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr_tuned, tpr_tuned, thresholds_tuned = roc_curve(y_test, y_pred_prob_tuned)\n",
    "\n",
    "# Compute AUC score\n",
    "auc_tuned = roc_auc_score(y_test, y_pred_prob_tuned)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_tuned, tpr_tuned, color='blue', label='ROC Curve (Tuned) (AUC = {:.2f})'.format(auc_tuned))\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Diagonal line for random guessing\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Tuned RandomForest')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print the AUC\n",
    "print(\"AUC Score (Tuned):\", auc_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assume y_test and y_pred are defined elsewhere in your code\n",
    "# Compute the confusion matrix\n",
    "cm_rfc_tuned = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define the labels from your dataset (adjust labels as per your actual data)\n",
    "labels = ['BENIGN', 'DoS', 'PortScan', 'DDoS', 'Brute Force', 'Web Attack', 'Bot', 'Infiltration', 'Heartbleed']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_rfc_tuned, annot=True, cmap='Blues', fmt='g',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost model\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "mlflow.set_experiment(\"training_by_default_multiclass\")\n",
    "mlflow.start_run(run_name=\"xgb_multi_model\",nested=True)\n",
    "# Fit the RandomForest model on the training data\n",
    "xgb.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  cross_validate\n",
    "\n",
    "# Define K-Fold cross-validator\n",
    "kfold_validation = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the classifier\n",
    "xgb_cross = XGBClassifier(random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation with multiple scoring metrics\n",
    "xgb_cross_scores = cross_validate(xgb_cross, X, y, cv=kfold_validation, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Print the scores for each fold and metric\n",
    "print(\"Average Accuracy Score:\", xgb_cross_scores['test_score'].mean())\n",
    "print(xgb_cross_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = xgb.predict(X_test_final)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "score.append(['XGB', accuracy, recall, precision, f1, mcc])\n",
    "\n",
    "metrics = {\"accuracy\" : accuracy , \"recall\" :recall, \"precision\":precision,\"f1\":f1,\"mcc\":mcc}\n",
    "mlflow.log_metrics(metrics)\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy * 100)\n",
    "print('Recall:', recall * 100)\n",
    "print('Precision:', precision * 100)\n",
    "print('F1 Score:', f1 * 100)\n",
    "print('MCC:', mcc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assume y_test and y_pred are defined elsewhere in your code\n",
    "# Compute the confusion matrix\n",
    "cm_xgb = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define the labels from your dataset (adjust labels as per your actual data)\n",
    "labels = ['BENIGN', 'DoS', 'PortScan', 'DDoS', 'Brute Force', 'Web Attack', 'Bot', 'Infiltration', 'Heartbleed']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_xgb, annot=True, cmap='Blues', fmt='g',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Set the MLflow tracking URI and experiment\n",
    "mlflow.set_tracking_uri(\"http://localhost:5001\")  # Ensure this URI is correct\n",
    "mlflow.set_experiment(\"hyperparameter_tuning_multiclass\")\n",
    "\n",
    "# Define parameter grid for XGBoost\n",
    "param_dist_xgb = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'gamma': [0, 0.1]\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV for XGBoost\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "    estimator=XGBClassifier(random_state=42),\n",
    "    param_distributions=param_dist_xgb,\n",
    "    n_iter=50,\n",
    "    scoring='f1_weighted',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Ensure any existing runs are closed\n",
    "mlflow.end_run()\n",
    "\n",
    "# Start a new MLflow run within a context manager\n",
    "mlflow.start_run(run_name=\"xgb_tuned_multiclass\")\n",
    "# Fit random search to the data\n",
    "random_search_xgb.fit(X_train_final, y_train)\n",
    "\n",
    "# Retrieve and log the best parameters\n",
    "best_params = random_search_xgb.best_params_\n",
    "print(\"Best Hyperparameters for XGBoost:\", best_params)\n",
    "mlflow.log_params(best_params)\n",
    "\n",
    "# Optionally log the model\n",
    "mlflow.sklearn.log_model(random_search_xgb.best_estimator_, \"xgb_model_mlflow\")\n",
    "\n",
    "# Save the best parameters to a JSON file (optional, not recommended within the context block)\n",
    "model_name = \"XGB\"\n",
    "save_best_params_to_json(model_name, best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best XGBoost model on the test set\n",
    "best_xgb = random_search_xgb.best_estimator_\n",
    "y_pred = best_xgb.predict(X_test_final)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "score_hyper.append(['XGB (Tuned)', accuracy, recall, precision, f1, mcc])\n",
    "metrics = {\"accuracy\" : accuracy , \"recall\" :recall, \"precision\":precision,\"f1\":f1,\"mcc\":mcc}\n",
    "mlflow.log_metrics(metrics)\n",
    "mlflow.end_run()\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy * 100)\n",
    "print('Recall:', recall * 100)\n",
    "print('Precision:', precision * 100)\n",
    "print('F1 Score:', f1 * 100)\n",
    "print('MCC:', mcc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) are important tools for evaluating the performance of a binary classifier. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings, providing insight into the trade-off between sensitivity (true positive rate) and specificity (false positive rate). The AUC provides a single scalar value which measures the overall ability of the model to discriminate between the positive and negative classes across all possible thresholds. Essentially, a higher AUC value indicates better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_xgb = best_xgb.predict_proba(X_test_final)[:, 1]\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_prob_xgb)\n",
    "auc_xgb = roc_auc_score(y_test, y_pred_prob_xgb)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_xgb, tpr_xgb, color='green', label='ROC Curve (XGBoost) (AUC = {:.2f})'.format(auc_xgb))\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Tuned XGBoost')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assume y_test and y_pred are defined elsewhere in your code\n",
    "# Compute the confusion matrix\n",
    "cm_xgb_tuned = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define the labels from your dataset (adjust labels as per your actual data)\n",
    "labels = ['BENIGN', 'DoS', 'PortScan', 'DDoS', 'Brute Force', 'Web Attack', 'Bot', 'Infiltration', 'Heartbleed']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_xgb_tuned, annot=True, cmap='Blues', fmt='g',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Decision Tree model\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "mlflow.set_experiment(\"training_by_default_multiclass\")\n",
    "mlflow.start_run(run_name=\"decision_tree\",nested=True)\n",
    "    # Fit the RandomForest model on the training data\n",
    "dt.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k- fold cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  cross_validate\n",
    "\n",
    "# Define K-Fold cross-validator\n",
    "kfold_validation = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the classifier\n",
    "DT_cross = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation with multiple scoring metrics\n",
    "DT_cross_scores = cross_validate(DT_cross, X, y, cv=kfold_validation, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Print the scores for each fold and metric\n",
    "print(\"Average Accuracy Score:\", DT_cross_scores['test_score'].mean())\n",
    "print(DT_cross_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred_dt = dt.predict(X_test_final)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "score.append(['Decision Tree ', accuracy, recall, precision, f1, mcc])\n",
    "\n",
    "metrics = {\"accuracy\" : accuracy , \"recall\" :recall, \"precision\":precision,\"f1\":f1,\"mcc\":mcc}\n",
    "mlflow.log_metrics(metrics)\n",
    "mlflow.end_run()\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy * 100)\n",
    "print('Recall:', recall * 100)\n",
    "print('Precision:', precision * 100)\n",
    "print('F1 Score:', f1 * 100)\n",
    "print('MCC:', mcc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assume y_test and y_pred are defined elsewhere in your code\n",
    "# Compute the confusion matrix\n",
    "cm_dt = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define the labels from your dataset (adjust labels as per your actual data)\n",
    "labels = ['BENIGN', 'DoS', 'PortScan', 'DDoS', 'Brute Force', 'Web Attack', 'Bot', 'Infiltration', 'Heartbleed']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_dt, annot=True, cmap='Blues', fmt='g',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define parameter grid for Decision Tree\n",
    "param_dist_dt = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'splitter': ['best', 'random']\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV for Decision Tree\n",
    "random_search_dt = RandomizedSearchCV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    param_distributions=param_dist_dt,\n",
    "    n_iter=50,  # Adjusted number of iterations\n",
    "    scoring='f1_weighted',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Set the MLflow tracking URI and experiment\n",
    "mlflow.set_tracking_uri(\"http://localhost:5001\")  # Ensure this URI is correct\n",
    "mlflow.set_experiment(\"hyperparameter_tuning_multiclass\")\n",
    "\n",
    "# Ensure any existing runs are closed\n",
    "mlflow.end_run()\n",
    "\n",
    "# Start a new MLflow run within a context manager\n",
    "mlflow.start_run(run_name=\"decision_tree_tuned_multiclass\")\n",
    "# Fit random search to the data\n",
    "random_search_dt.fit(X_train_final, y_train)\n",
    "\n",
    "# Retrieve and log the best parameters\n",
    "best_params = random_search_dt.best_params_\n",
    "print(\"Best Hyperparameters for Decision Tree:\", best_params)\n",
    "mlflow.log_params(best_params)\n",
    "\n",
    "# Optionally log the model\n",
    "mlflow.sklearn.log_model(random_search_dt.best_estimator_, \"Decision_Tree_model_mlflow\")\n",
    "\n",
    "# Save the best parameters to a JSON file (optional, not recommended within the context block)\n",
    "model_name = \"Decision Tree\"\n",
    "save_best_params_to_json(model_name, best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dt = random_search_dt.best_estimator_\n",
    "y_pred = best_dt.predict(X_test_final)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "score_hyper.append(['Decision Tree (Tuned)', accuracy, recall, precision, f1, mcc])\n",
    "metrics = {\"accuracy\" : accuracy , \"recall\" :recall, \"precision\":precision,\"f1\":f1,\"mcc\":mcc}\n",
    "mlflow.log_metrics(metrics)\n",
    "mlflow.end_run()\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy * 100)\n",
    "print('Recall:', recall * 100)\n",
    "print('Precision:', precision * 100)\n",
    "print('F1 Score:', f1 * 100)\n",
    "print('MCC:', mcc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) are important tools for evaluating the performance of a binary classifier. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings, providing insight into the trade-off between sensitivity (true positive rate) and specificity (false positive rate). The AUC provides a single scalar value which measures the overall ability of the model to discriminate between the positive and negative classes across all possible thresholds. Essentially, a higher AUC value indicates better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_dt = best_dt.predict_proba(X_test_final)[:, 1]\n",
    "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_prob_dt)\n",
    "auc_dt = roc_auc_score(y_test, y_pred_prob_dt)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_dt, tpr_dt, color='purple', label='ROC Curve (Decision Tree) (AUC = {:.2f})'.format(auc_dt))\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Tuned Decision Tree')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assume y_test and y_pred are defined elsewhere in your code\n",
    "# Compute the confusion matrix\n",
    "cm_dt_tuned = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define the labels from your dataset (adjust labels as per your actual data)\n",
    "labels = ['BENIGN', 'DoS', 'PortScan', 'DDoS', 'Brute Force', 'Web Attack', 'Bot', 'Infiltration', 'Heartbleed']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_dt_tuned, annot=True, cmap='Blues', fmt='g',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Logistic Regression: While simpler, logistic regression can still provide solid results, especially when regularized using techniques like L2 (Ridge) or L1 (Lasso)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "lr = LogisticRegression(random_state=42)\n",
    "mlflow.set_experiment(\"training_by_default_multiclass\")\n",
    "\n",
    "mlflow.start_run(run_name=\"logistic_regression\",nested=True)\n",
    "    # Fit the RandomForest model on the training data\n",
    "lr.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  cross_validate\n",
    "\n",
    "# Define K-Fold cross-validator\n",
    "kfold_validation = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the classifier\n",
    "lr_cross = LogisticRegression(random_state=42)\n",
    "\n",
    "# Perform k-fold cross-validation with multiple scoring metrics\n",
    "lr_cross_scores = cross_validate(lr_cross, X, y, cv=kfold_validation, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Print the scores for each fold and metric\n",
    "print(\"Average Accuracy Score:\", lr_cross_scores['test_score'].mean())\n",
    "print(lr_cross_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = lr.predict(X_test_final)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "score.append(['Logistic Regression', accuracy, recall, precision, f1, mcc])\n",
    "\n",
    "metrics = {\"accuracy\" : accuracy , \"recall\" :recall, \"precision\":precision,\"f1\":f1,\"mcc\":mcc }\n",
    "mlflow.log_metrics(metrics)\n",
    "mlflow.end_run()\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy * 100)\n",
    "print('Recall:', recall * 100)\n",
    "print('Precision:', precision * 100)\n",
    "print('F1 Score:', f1 * 100)\n",
    "print('MCC:', mcc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assume y_test and y_pred are defined elsewhere in your code\n",
    "# Compute the confusion matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define the labels from your dataset (adjust labels as per your actual data)\n",
    "labels = ['BENIGN', 'DoS', 'PortScan', 'DDoS', 'Brute Force', 'Web Attack', 'Bot', 'Infiltration', 'Heartbleed']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_lr, annot=True, cmap='Blues', fmt='g',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for Logistic Regression\n",
    "param_grid = {\n",
    "    'penalty': ['l2'],           # Simplified to only 'l2' for now to reduce complexity\n",
    "    'C': [0.1, 1.0],             # Reduced range for C\n",
    "    'solver': ['liblinear'],     # Using 'liblinear' as it's often faster for smaller datasets\n",
    "    'max_iter': [1000],          # Standard max_iter to see if it converges\n",
    "    'tol': [1e-4]                # Standard tolerance\n",
    "}\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "clf = GridSearchCV(log_model, param_grid=param_grid, cv=3, verbose=True, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "# Set the MLflow tracking URI and experiment\n",
    "mlflow.set_tracking_uri(\"http://localhost:5001\")  # Ensure this URI is correct\n",
    "mlflow.set_experiment(\"hyperparameter_tuning_multiclass\")\n",
    "\n",
    "# Ensure any existing runs are closed\n",
    "mlflow.end_run()\n",
    "\n",
    "# Start a new MLflow run within a context manager\n",
    "mlflow.start_run(run_name=\"logistic_regression_tuned_multiclass\")\n",
    "# Fit GridSearchCV to the data\n",
    "clf.fit(X_train_final, y_train)\n",
    "\n",
    "# Retrieve and log the best parameters\n",
    "best_params = clf.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "print(\"Best cross-validated score: \", clf.best_score_)\n",
    "\n",
    "# Log the best parameters\n",
    "mlflow.log_params(best_params)\n",
    "\n",
    "# Optionally log the model\n",
    "mlflow.sklearn.log_model(clf.best_estimator_, \"logistic_regression_model_mlflow\")\n",
    "\n",
    "# Save the best parameters to a JSON file\n",
    "model_name = \"Logistic Regression\"\n",
    "save_best_params_to_json(model_name, best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from GridSearchCV\n",
    "best_lr = clf.best_estimator_\n",
    "\n",
    "# Validating the model with training data (optional)\n",
    "train_score = best_lr.score(X_train_final, y_train)\n",
    "print(f'Accuracy on training set: {train_score:.3f}')\n",
    "\n",
    "# Testing the best logistic regression model (after hyperparameter tuning)\n",
    "y_pred = best_lr.predict(X_test_final)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Append the results to the hyperparameter-tuned score list\n",
    "score_hyper.append(['Logistic Regression (Tuned)', accuracy, recall, precision, f1, mcc])\n",
    "metrics = {\"accuracy\" : accuracy , \"recall\" :recall, \"precision\":precision,\"f1\":f1,\"mcc\":mcc }\n",
    "mlflow.log_metrics(metrics)\n",
    "# End the MLflow run\n",
    "mlflow.end_run()\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n",
    "print(f\"MCC: {mcc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) are important tools for evaluating the performance of a binary classifier. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings, providing insight into the trade-off between sensitivity (true positive rate) and specificity (false positive rate). The AUC provides a single scalar value which measures the overall ability of the model to discriminate between the positive and negative classes across all possible thresholds. Essentially, a higher AUC value indicates better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_lr = best_lr.predict_proba(X_test_final)[:, 1]\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_prob_lr)\n",
    "auc_lr = roc_auc_score(y_test, y_pred_prob_lr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_lr, tpr_lr, color='orange', label='ROC Curve (Logistic Regression) (AUC = {:.2f})'.format(auc_lr))\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Tuned Logistic Regression')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assume y_test and y_pred are defined elsewhere in your code\n",
    "# Compute the confusion matrix\n",
    "cm_lr_tuned = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define the labels from your dataset (adjust labels as per your actual data)\n",
    "labels = ['BENIGN', 'DoS', 'PortScan', 'DDoS', 'Brute Force', 'Web Attack', 'Bot', 'Infiltration', 'Heartbleed']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_lr_tuned, annot=True, cmap='Blues', fmt='g',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Naive Bayes: This model is quick and easy to implement but may be limited by its assumptions of feature independence.\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Naive Bayes model\n",
    "nb = GaussianNB()\n",
    "mlflow.set_experiment(\"training_by_default_multiclass\")\n",
    "\n",
    "\n",
    "mlflow.start_run(run_name=\"naive_multi_bayes\",nested=True)\n",
    "    # Fit the RandomForest model on the training data\n",
    "nb.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  cross_validate\n",
    "\n",
    "# Define K-Fold cross-validator\n",
    "kfold_validation = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the classifier\n",
    "nb_cross = GaussianNB()\n",
    "\n",
    "# Perform k-fold cross-validation with multiple scoring metrics\n",
    "nb_cross_scores = cross_validate(nb_cross, X, y, cv=kfold_validation, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Print the scores for each fold and metric\n",
    "print(\"Average Accuracy Score:\", nb_cross_scores['test_score'].mean())\n",
    "print(nb_cross_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = nb.predict(X_test_final)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "score.append(['Naive Bayes', accuracy, recall, precision, f1, mcc])\n",
    "\n",
    "metrics = {\"accuracy\" : accuracy , \"recall\" :recall, \"precision\":precision,\"f1\":f1,\"mcc\":mcc, }\n",
    "mlflow.log_metrics(metrics)\n",
    "mlflow.end_run()\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy * 100)\n",
    "print('Recall:', recall * 100)\n",
    "print('Precision:', precision * 100)\n",
    "print('F1 Score:', f1 * 100)\n",
    "print('MCC:', mcc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assume y_test and y_pred are defined elsewhere in your code\n",
    "# Compute the confusion matrix\n",
    "cm_naive_bayes = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define the labels from your dataset (adjust labels as per your actual data)\n",
    "labels = ['BENIGN', 'DoS', 'PortScan', 'DDoS', 'Brute Force', 'Web Attack', 'Bot', 'Infiltration', 'Heartbleed']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_naive_bayes, annot=True, cmap='Blues', fmt='g',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define parameter grid for Gaussian Naive Bayes\n",
    "param_dist_nb = {\n",
    "    'var_smoothing': [1e-12, 1e-11, 1e-10, 1e-09, 1e-08, 1e-07, 1e-06]\n",
    "\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV for Gaussian Naive Bayes\n",
    "random_search_nb = RandomizedSearchCV(\n",
    "    estimator=GaussianNB(),\n",
    "    param_distributions=param_dist_nb,\n",
    "    n_iter=7,  # Adjusted number of iterations due to fewer parameters\n",
    "    scoring='f1_weighted',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Set the MLflow tracking URI and experiment\n",
    "mlflow.set_tracking_uri(\"http://localhost:5001\")  # Ensure this URI is correct\n",
    "mlflow.set_experiment(\"hyperparameter_tuning\")\n",
    "\n",
    "# Ensure any existing runs are closed\n",
    "mlflow.end_run()\n",
    "\n",
    "# Start a new MLflow run within a context manager\n",
    "mlflow.start_run(run_name=\"naive_bayes_tuned\")\n",
    "# Fit RandomizedSearchCV to the data\n",
    "random_search_nb.fit(X_train_final, y_train)\n",
    "\n",
    "# Retrieve and log the best parameters\n",
    "best_params = random_search_nb.best_params_\n",
    "print(\"Best Hyperparameters for Gaussian Naive Bayes:\", best_params)\n",
    "mlflow.log_params(best_params)\n",
    "\n",
    "# Optionally log the model\n",
    "mlflow.sklearn.log_model(random_search_nb.best_estimator_, \"Naive_Bayes_model_mlflow\")\n",
    "\n",
    "# Save the best parameters to a JSON file (optional, not recommended within the context block)\n",
    "model_name = \"Naive Bayes\"\n",
    "save_best_params_to_json(model_name, best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best Naive Bayes model on the test set\n",
    "best_nb = random_search_nb.best_estimator_\n",
    "y_pred_nb = best_nb.predict(X_test_final)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Append to the score_hyper list\n",
    "score_hyper.append(['Naive bayes (Tuned)', accuracy, recall, precision, f1, mcc])\n",
    "metrics = {\"accuracy\" : accuracy , \"recall\" :recall, \"precision\":precision,\"f1\":f1,\"mcc\":mcc}\n",
    "mlflow.log_metrics(metrics)\n",
    "mlflow.end_run()\n",
    "# Print the results\n",
    "print(\"Accuracy:\", accuracy * 100)\n",
    "print('Recall:', recall * 100)\n",
    "print('Precision:', precision * 100)\n",
    "print('F1 Score:', f1 * 100)\n",
    "print('MCC:', mcc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC) are important tools for evaluating the performance of a binary classifier. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings, providing insight into the trade-off between sensitivity (true positive rate) and specificity (false positive rate). The AUC provides a single scalar value which measures the overall ability of the model to discriminate between the positive and negative classes across all possible thresholds. Essentially, a higher AUC value indicates better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_nb = best_nb.predict_proba(X_test_final)[:, 1]\n",
    "fpr_nb, tpr_nb, _ = roc_curve(y_test, y_pred_prob_nb)\n",
    "auc_nb = roc_auc_score(y_test, y_pred_prob_nb)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_nb, tpr_nb, color='cyan', label='ROC Curve (Naive Bayes) (AUC = {:.2f})'.format(auc_nb))\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Tuned Naive Bayes')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assume y_test and y_pred are defined elsewhere in your code\n",
    "# Compute the confusion matrix\n",
    "cm_nb_tunned = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define the labels from your dataset (adjust labels as per your actual data)\n",
    "labels = ['BENIGN', 'DoS', 'PortScan', 'DDoS', 'Brute Force', 'Web Attack', 'Bot', 'Infiltration', 'Heartbleed']\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_nb_tunned, annot=True, cmap='Blues', fmt='g',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames for Model Metrics:\n",
    "\n",
    "### After collecting the results in score and score_hyper , I have created a new DataFrames to display the results (accuracy, recall, etc.), as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for the tables\n",
    "columns = ['Model', 'Accuracy', 'Recall', 'Precision', 'F1 Score', 'MCC']\n",
    "\n",
    "# Create a DataFrame for the normal training results\n",
    "df_normal = pd.DataFrame(score, columns=columns)\n",
    "\n",
    "# Create a DataFrame for the hyperparameter tuning results\n",
    "df_hyper = pd.DataFrame(score_hyper, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normal Training Results:\")\n",
    "df_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Normal Training Results:\")\n",
    "df_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and Save Multiple Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the confusion matrices and titles in tuples for easier handling\n",
    "confusion_matrices = [\n",
    "    (cm_rfc_tuned, \"Random Forest\"),\n",
    "    (cm_xgb_tuned, \"XGBoost\"),\n",
    "    (cm_lr_tuned, \"Logistic Regression\"),\n",
    "    (cm_dt_tuned, \"Decision Tree\"),\n",
    "    (cm_nb_tunned, \"Naive Bayes\")\n",
    "]\n",
    "\n",
    "# Setup the matplotlib figure and axes\n",
    "num_matrices = len(confusion_matrices)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=num_matrices, figsize=(5 * num_matrices, 5))  # Adjust size as needed\n",
    "\n",
    "# Check if only one row of subplots, make axes iterable\n",
    "if num_matrices == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Loop through the confusion matrices and corresponding axes\n",
    "for ax, (cm, title) in zip(axes, confusion_matrices):\n",
    "    # Plotting the confusion matrix using seaborn's heatmap function\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', ax=ax, \n",
    "                xticklabels=['BENIGN', 'Attacks'], yticklabels=['BENIGN', 'Attacks'])\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "\n",
    "# Tight layout to ensure subplots do not overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('all_confusion_matrices_multiclass.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the cross-validation score data is correct as provided\n",
    "models = ['Naive Bayes', 'Logistic Regression', 'Decision Tree', 'XGBoost', 'Random Forest']\n",
    "results = {\n",
    "    'Model': models,\n",
    "    'Fold-1': [scores['test_score'][0] for scores in [nb_cross_scores, lr_cross_scores, DT_cross_scores, xgb_cross_scores, rf_cross_scores]],\n",
    "    'Fold-2': [scores['test_score'][1] for scores in [nb_cross_scores, lr_cross_scores, DT_cross_scores, xgb_cross_scores, rf_cross_scores]],\n",
    "    'Fold-3': [scores['test_score'][2] for scores in [nb_cross_scores, lr_cross_scores, DT_cross_scores, xgb_cross_scores, rf_cross_scores]],\n",
    "    'Fold-4': [scores['test_score'][3] for scores in [nb_cross_scores, lr_cross_scores, DT_cross_scores, xgb_cross_scores, rf_cross_scores]],\n",
    "    'Fold-5': [scores['test_score'][4] for scores in [nb_cross_scores, lr_cross_scores, DT_cross_scores, xgb_cross_scores, rf_cross_scores]]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Avoid converting to strings, calculate mean and standard deviation directly\n",
    "df_results['Mean Accuracy'] = df_results[['Fold-1', 'Fold-2', 'Fold-3', 'Fold-4', 'Fold-5']].mean(axis=1)\n",
    "df_results['Standard Deviation'] = df_results[['Fold-1', 'Fold-2', 'Fold-3', 'Fold-4', 'Fold-5']].std(axis=1)\n",
    "\n",
    "# Formatting the output for display (not altering the actual DataFrame)\n",
    "formatted_df = df_results.copy()\n",
    "formatted_df.iloc[:, 1:] = formatted_df.iloc[:, 1:].map(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# Creating the table\n",
    "table = ax.table(cellText=formatted_df.values, colLabels=formatted_df.columns, cellLoc='center', loc='center', colWidths=[0.15, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.13])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)\n",
    "\n",
    "# Add a title\n",
    "plt.title('Cross-Validation Results for Various Models')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('model_performance_table_multiclass.png')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save All ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.plot(fpr_tuned, tpr_tuned, color='blue', label='Random Forest (AUC = {:.2f})'.format(auc_tuned))\n",
    "plt.plot(fpr_xgb, tpr_xgb, color='green', label='XGBoost (AUC = {:.2f})'.format(auc_xgb))\n",
    "plt.plot(fpr_dt, tpr_dt, color='purple', label='Decision Tree (AUC = {:.2f})'.format(auc_dt))\n",
    "plt.plot(fpr_lr, tpr_lr, color='orange', label='Logistic Regression (AUC = {:.2f})'.format(auc_lr))\n",
    "plt.plot(fpr_nb, tpr_nb, color='cyan', label='Naive Bayes (AUC = {:.2f})'.format(auc_nb))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Random chance\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Comparison of ROC Curves for All Tuned Models')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the combined plot\n",
    "plt.savefig('all_roc_curves.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
