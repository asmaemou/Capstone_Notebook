{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the datasets into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fridayAF_DDos = pd.read_csv(\"./datasets/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")\n",
    "df_fridayAF_PortScan = pd.read_csv(\"./datasets/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\")\n",
    "df_fridayMO = pd.read_csv(\"./datasets/Friday-WorkingHours-Morning.pcap_ISCX.csv\")\n",
    "df_monday = pd.read_csv(\"./datasets/Monday-WorkingHours.pcap_ISCX.csv\")\n",
    "df_thursdayAF_Infilteration = pd.read_csv(\"./datasets/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\")\n",
    "df_thursdayMO_WebAttacks = pd.read_csv(\"./datasets/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\")\n",
    "df_tuesday = pd.read_csv(\"./datasets/Tuesday-WorkingHours.pcap_ISCX.csv\")\n",
    "df_wednesday = pd.read_csv(\"./datasets/Wednesday-workingHours.pcap_ISCX.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating the dataframes to single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.concat([df_fridayAF_DDos,df_fridayAF_PortScan,df_fridayMO,df_monday,df_thursdayAF_Infilteration,df_thursdayMO_WebAttacks,df_tuesday,df_wednesday], axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find columns with zero variance; columns where all values are the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_variance_cols = [col for col in df_data.columns if df_data[col].nunique() == 1]\n",
    "\n",
    "# Display the columns with zero variance\n",
    "if zero_variance_cols:\n",
    "    print(f\"Columns with zero variance: {zero_variance_cols}\")\n",
    "else:\n",
    "    print(\"No columns with zero variance found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shape before removing zero variance columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape before removing zero variance columns:', df_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle columns with zero variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if zero_variance_cols:\n",
    "        df_data.drop(zero_variance_cols, axis=1, inplace=True)\n",
    "        print(f'Dropped zero variance columns: {zero_variance_cols}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shape after removing zero variance columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape after removing zero variance columns:', df_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find  spaces from column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handle spaces from column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.columns = df_data.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns after removing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Rows with NaN, inf, or -inf Values\n",
    "##### The row listed here contain  NaN across the columns displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options to handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Option1: Drop rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_cleaned = df_data.dropna(subset=['Flow Bytes/s'])\n",
    "\n",
    "# Check shape after removing rows with missing values\n",
    "print(\"Shape after dropping rows with missing 'Flow Bytes/s':\", df_data_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Fill Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fill with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data['Flow Bytes/s'].fillna(df_data['Flow Bytes/s'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data['Flow Bytes/s'].fillna(df_data['Flow Bytes/s'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data['Flow Bytes/s'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The row listed here contain inf, or -inf value across the columns displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "numeric_cols = df_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Check for inf and -inf values in the numeric columns\n",
    "inf_values = np.isinf(numeric_cols).sum()\n",
    "\n",
    "# Display the count of inf and -inf values in each numeric column\n",
    "print(\"Count of inf and -inf values in each numeric column:\")\n",
    "print(inf_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = df_data[df_data.duplicated()]\n",
    "\n",
    "if not duplicate_rows.empty:\n",
    "    print(\"Duplicate rows:\")\n",
    "    print(duplicate_rows)\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify columns with identical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_pairs = []\n",
    "num_columns = len(df_data.columns)\n",
    "\n",
    "for i in range(num_columns):\n",
    "    for j in range(i + 1, num_columns):\n",
    "        if df_data.iloc[:, i].equals(df_data.iloc[:, j]): \n",
    "            column_pairs.append((df_data.columns[i], df_data.columns[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape before removing identical columns:\", df_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print the column pairs with identical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if column_pairs:\n",
    "    print(\"Columns with identical values:\")\n",
    "    for pair in column_pairs:\n",
    "        print(f\"{pair[0]} and {pair[1]} have identical values.\")\n",
    "    \n",
    "    # Step 3: Drop one column from each pair\n",
    "    columns_to_drop = [pair[1] for pair in column_pairs]\n",
    "    df_data.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "    print(f\"Dropped columns: {columns_to_drop}\")\n",
    "else:\n",
    "    print(\"No columns with identical values found.\")\n",
    "\n",
    "print(\"Shape after removing identical columns:\", df_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for non-numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_columns = df_data.select_dtypes(include=['object']).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Label', data=df_data)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['Label'] = df_data['Label'].map({\n",
    "    'BENIGN': 0,              \n",
    "    'DDoS': 1,                \n",
    "    'PortScan': 1,\n",
    "    'Bot': 1,\n",
    "    'Infiltration': 1,\n",
    "    'Web Attack ': 1,\n",
    "    'Brute Force': 1,\n",
    "    'Web Attack � XSS': 1,\n",
    "    'Web Attack � Sql Injection': 1,\n",
    "    'FTP-Patator': 1,\n",
    "    'SSH-Patator': 1,\n",
    "    'DoS slowloris': 1,\n",
    "    'DoS Slowhttptest': 1,\n",
    "    'DoS Hulk': 1,\n",
    "    'DoS GoldenEye': 1,\n",
    "    'Heartbleed': 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Label', data=df_data)\n",
    "plt.xticks(rotation=45, ha='right') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object typically refers to string or mixed data types\n",
    "### The output usually includes:\n",
    "\n",
    "#### count: Number of non-null entries in each column.\n",
    "#### unique: Number of unique values in each column.\n",
    "#### top: Most frequent value in each column.\n",
    "#### freq: Frequency of the most common value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of duplicate rows: {df_data.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
